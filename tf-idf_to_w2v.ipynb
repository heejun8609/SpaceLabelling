{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시청역\n",
      "을지로입구역\n",
      "을지로3가역\n",
      "을지로4가역\n",
      "동대문역사문화공원역\n",
      "신당역\n",
      "상왕십리역\n",
      "왕십리역\n",
      "한양대역\n",
      "뚝섬역\n",
      "성수역\n",
      "건대입구역\n",
      "구의역\n",
      "강변역\n",
      "잠실나루역\n",
      "잠실역\n",
      "신천역\n",
      "종합운동장역\n",
      "삼성역\n",
      "선릉역\n",
      "역삼역\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Twitter\n",
    "import os\n",
    "from gensim import corpora, models, similarities\n",
    "import numpy as np\n",
    "\n",
    "station = pd.read_excel(\"2호선 역 목록.xlsx\")\n",
    "# 역 이름에 # 붙여서 리스트로 저장\n",
    "station = list(station['총'])\n",
    "stations = [x for x in station if str(x) != 'nan']\n",
    "stations\n",
    "\n",
    "w2v_list = []\n",
    "# 포스팅 분리\n",
    "for station in stations:\n",
    "    try:\n",
    "        doc = open('crawl/'+station + '.csv', 'r')\n",
    "        data = doc.read()\n",
    "        doc.close()\n",
    "        phrase = data.split('\\n')\n",
    "    except:\n",
    "        doc = open('crawl/'+station  + '.csv', 'r', encoding='utf8')\n",
    "        data = doc.read()\n",
    "        doc.close()\n",
    "        phrase = data.split('\\n')\n",
    "        continue\n",
    "\n",
    "    # 포스팅에서 한글 외 문자 제거\n",
    "    import re\n",
    "\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ가-힣]+')\n",
    "\n",
    "    # 형태소 문장 단위 추출\n",
    "    dic_Twitter = []\n",
    "    for i in range(len(phrase)):\n",
    "        p = hangul.sub(' ', phrase[i])\n",
    "        s = Twitter().pos(p, stem=True, norm=True)\n",
    "        dic_Twitter.append(s)\n",
    "\n",
    "    # 총 형태소\n",
    "    name = doc.name\n",
    "    dic_ad_noun=[]\n",
    "    for twit in dic_Twitter:\n",
    "        c = [x[0] for x in twit if x[1] ==\"Adjective\" or x[1] == \"Noun\" and len(x[0]) >1 and x[0] not in [\"스타\",\"그램\",name[:-4],name[:-5],\"이다\",\"있다\",\"없다\",\"같다\",\"아니다\",\"그렇다\",\"이렇다\",\"많다\",\"어떻다\",\n",
    "                                             \"스럽다\",\"저렇다\",\"인스타그램\",\"맞팔\",\"선팔\",\"이벤트\",\"응모\"]]\n",
    "        dic_ad_noun.append(c)\n",
    "    x=np.str(station)\n",
    "    y= models.Word2Vec(dic_ad_noun, size=100, min_count=1)\n",
    "    w2v={x:y}\n",
    "    w2v_list.append(w2v)\n",
    "    print(station)\n",
    "print(\"1차 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ps_dic=[]\n",
    "for station in stations:\n",
    "    file = pd.read_excel('역별 tf-idf.xlsx')\n",
    "    result = pd.DataFrame(np.zeros(20))\n",
    "    words = list(file.ix[:, station])\n",
    "    new_words = [x for x in words if str(x) != 'nan']\n",
    "    ps_dic.append({station:new_words})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#w2v을 역별로 형용사 15씩 적용하기\n",
    "w2v_ps_dic=[]\n",
    "for i, sta in enumerate(stations):\n",
    "    try:\n",
    "        kk=w2v_list[i][sta]\n",
    "        w2v_dic =[]\n",
    "        for a in ps_dic[i][stations[i]]:\n",
    "            wps={a:kk.similar_by_word(a,topn=10)}\n",
    "            w2v_dic.append(wps)\n",
    "        w2v_ps_dic.append((sta,w2v_dic))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for v in range(len(stations)):\n",
    "    x= pd.DataFrame(w2v_ps_dic[v][1][0])\n",
    "    print(x)\n",
    "    for i in range(len(w2v_ps_dic[v][1])):\n",
    "        x= pd.concat([x,pd.DataFrame(w2v_ps_dic[v][1][i+1])], axis=1)\n",
    "    x.to_excel(stations[v]+'_tf-idf_to_w2v.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for v in range(50):\n",
    "    x= pd.DataFrame(w2v_ps_dic[v][1][0])\n",
    "    print(x)\n",
    "    for i in range(14):\n",
    "        print(i+1)\n",
    "        x= pd.concat([x,pd.DataFrame(w2v_ps_dic[v][1][i+1])], axis=1)\n",
    "        x.to_excel(new_station_list[v]+'.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
